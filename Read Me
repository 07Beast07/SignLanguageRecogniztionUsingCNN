This project shows Camera Vision based techniques used for image analysis. The latest technological generation is capable of 
executing complex image processing applications, with the most penalizing factor being camera. The project focuses on developing 
a sign language recognition for detecting sign language with English alphabets (A-Z) and subsequently convert it to text. 



1. The first thing done was downloading 27 gesture samples. For each gesture there are 1200 images which were 50x50 pixels. 
   All theses images were in grayscale which is stored in the gestures/ folder. The pictures are flipped as well. 
   Hence each gesture has 2400 images.
   Download the dataset from https://drive.google.com/drive/folders/1YDc4-jk6VOpYAZsGdE44_RSawKwN2I8F?usp=sharing
   
2. Created a CNN which look a lot similar to <a href="https://www.tensorflow.org/tutorials/layers">this MNIST classifying model</a> using Keras. 
3. As of today, there are 27 gestures for which are 26 alphabets. And trained the model on these images.

There are a lot of details thats left. But these are the basic and main steps.


## Requirements
0. Python 3.x
1. <a href="https://tensorflow.org">Tensorflow 1.5</a>
2. <a href="https://keras.io">Keras</a>
3. OpenCV 3.4
4. h5py
5. pyttsx3
6. A good grasp over the above 5 topics along with neural networks. Refer to the internet if you have problems with those. I myself am just a begineer in those.
7. A good CPU (preferably with a GPU).
8. Patience.... A lot of it.

## Installing the requirements
1. Start your terminal of cmd depending on your os.
  2. If you have a NVidia GPU then make sure you have the prerequisites for Tensorflow GPU installation (Refer to official site). Then use this commmand

    pip install -r requirements_gpu.txt

  3. In case you do not have a GPU then use this command

    pip install -r requirements_cpu.txt

## How to use this repo
Before using this repo, let me warn about something. You will have no interactive interface that will tell you what to do. So you will have to figure out most of the stuff by yourself and also make some changes to the scripts if the needs arise. But here is a basic gist.


### Displaying all gestures
  1. To see all the gestures that are stored in 'gestures/' folder run this command
    
    python display_all_gestures.py

### Training a model
  1. So training can be done with Keras. Use the cnn_keras.py file.
  
    python cnn_keras.py

You will have the model in the root directory by the name cnn_model_keras2.h5.

You do not need to retrain your model every time. In case you added or removed a gesture then you need to retrain it.

### Get model reports
  1. To get the classification reports about the model make sure you have test_images and test_labels file which are generated by load_images.py. In case you do not have them run load_images.py file again. Then run this file

    python get_model_reports.py
  2. You will get the confusion matrix, f scores, precision and recall for the predictions by the model.

### Testing gestures
Before going into much details I would like to tell that I was not able to use the model trained using tensorflow. That is because I do not know how to use it. I tried using the predict() function of the Estimator API but that loads the parameters into memory every time it is called which is a huge overhead. Please help me if you can with this. The functions for prediction using tf is tf_predict() which you will find in the recognize_gesture.py file but it is never used.
This is why I ended up using Keras' model, as the loading the model into memory and using it for prediction is super easy.
   1. First set your hand histogram. 0. Watch the video guide for setting the hand histogram <a href='https://youtu.be/KYfBLeYDMW4'>here</a>. You do not need to do it again if you have already done it. But you do need to do it if the lighting conditions change. To do so type the command given below and follow the instructions below.
    
    python set_hand_hist.py

  * A windows "Set hand histogram" will appear.
  * "Set hand histogram" will have 50 squares (5x10).
  * Put your hand in those squares. Make sure your hand covers all the squares.
  * Press 'c'. 1 other window will appear "Thresh".
  * On pressing 'c' only white patches corresponding to the parts of the image which has your skin color should appear on the "Thresh" window. 
  * Make sure all the squares are covered by your hand.
  * In case you are not successful then move your hand a little bit and press 'c' again. Repeat this until you get a good histogram.
  * After you get a good histogram press 's' to save the histogram. All the windows close.
  2. For recognition start the recognize_gesture.py file.

    python recognize_gesture.py
3. You will have a small green box inside which you need to do your gestures.
